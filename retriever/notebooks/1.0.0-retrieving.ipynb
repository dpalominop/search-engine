{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpalominop/miniconda3/envs/haystack/lib/python3.8/site-packages/espnet2/gan_tts/vits/vits.py:43: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) >= LooseVersion(\"1.6.0\"):\n",
      "/home/dpalominop/miniconda3/envs/haystack/lib/python3.8/site-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n",
      "/home/dpalominop/miniconda3/envs/haystack/lib/python3.8/site-packages/elasticsearch/connection/http_urllib3.py:275: DeprecationWarning: HTTPResponse.getheaders() is deprecated and will be removed in urllib3 v2.1.0. Instead access HTTPResponse.headers directly.\n",
      "  return response.status, response.getheaders(), raw_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    host=host,\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    index=\"document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "querying_pipeline = Pipeline()\n",
    "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f2cd40e44543978de74479fd225068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = querying_pipeline.run(\n",
    "    query=\"What is BERT?\",\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 10},\n",
    "        \"Reader\": {\"top_k\": 5}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [<Answer {'answer': 'conceptually simple and empirically\\npowerful', 'type': 'extractive', 'score': 0.8382189869880676, 'context': 'l task-\\nspecific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven', 'offsets_in_document': [{'start': 893, 'end': 937}], 'offsets_in_context': [{'start': 53, 'end': 97}], 'document_id': 'c6a0b7e9bce962ba6d411032d8fde7ab', 'meta': {'_split_id': 0}}>,\n",
      "             <Answer {'answer': 'fine-\\ntuning approaches', 'type': 'extractive', 'score': 0.6779805421829224, 'context': 'tion to the architec-\\nture differences, BERT and OpenAI GPT are fine-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable e', 'offsets_in_document': [{'start': 326, 'end': 349}], 'offsets_in_context': [{'start': 64, 'end': 87}], 'document_id': 'b6d499fd36829a403510593004877b03', 'meta': {'_split_id': 57}}>,\n",
      "             <Answer {'answer': 'bidirectional self-attention', 'type': 'extractive', 'score': 0.6160318851470947, 'context': 'Critically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\nt', 'offsets_in_document': [{'start': 47, 'end': 75}], 'offsets_in_context': [{'start': 47, 'end': 75}], 'document_id': '2528eb809ff209042b0b5ca76de41806', 'meta': {'_split_id': 12}}>,\n",
      "             <Answer {'answer': 'the first fine-\\ntuning based representation model', 'type': 'extractive', 'score': 0.6068986654281616, 'context': 'y-engineered task-\\nspecific architectures. BERT is the first fine-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a l', 'offsets_in_document': [{'start': 942, 'end': 991}], 'offsets_in_context': [{'start': 51, 'end': 100}], 'document_id': '1ddff88697635ab760c1347ff3e63458', 'meta': {'_split_id': 4}}>,\n",
      "             <Answer {'answer': 'self-attention mechanism', 'type': 'extractive', 'score': 0.5107876062393188, 'context': 'Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npai', 'offsets_in_document': [{'start': 796, 'end': 820}], 'offsets_in_context': [{'start': 63, 'end': 87}], 'document_id': '2ed2006c8bd6ebf44a1659b6cef19aff', 'meta': {'_split_id': 19}}>],\n",
      " 'documents': [<Document: {'content': 'A.4 Comparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are fine-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n', 'content_type': 'text', 'score': 0.5412150102145328, 'meta': {'_split_id': 57}, 'embedding': None, 'id': 'b6d499fd36829a403510593004877b03'}>,\n",
      "               <Document: {'content': 'The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ([SEP]) and\\nclassifier token ([CLS]) which are only in-\\ntroduced at fine-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed-\\ndings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n• GPT used the same learning rate of 5e-5 for\\nall fine-tuning experiments; BERT chooses a\\ntask-specific fine-tuning learning rate which\\nperforms the best on the development set.\\nTo isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\n', 'content_type': 'text', 'score': 0.5382957293150809, 'meta': {'_split_id': 58}, 'embedding': None, 'id': 'cae0ebfdb523da1bfb63ef457ed2a290'}>,\n",
      "               <Document: {'content': 'Aligning books and movies: Towards\\nstory-like visual explanations by watching movies\\nand reading books. In Proceedings of the IEEE\\ninternational conference on computer vision, pages\\n19–27.\\nAppendix for “BERT: Pre-training of\\nDeep Bidirectional Transformers for\\nLanguage Understanding”\\nWe organize the appendix into three sections:\\n• Additional implementation details for BERT\\nare presented in Appendix A;\\n• Additional details for our experiments are\\npresented in Appendix B; and\\n• Additional ablation studies are presented in\\nAppendix C.\\nWe present additional ablation studies for\\nBERT including:\\n– Effect of Number of Training Steps; and\\n– Ablation for Different Masking Proce-\\ndures.\\nA Additional Details for BERT\\nA.1 Illustration of the Pre-training Tasks\\nWe provide examples of the pre-training tasks in\\nthe following.\\nMasked LM and the Masking Procedure As-\\nsuming the unlabeled sentence is my dog is\\nhairy, and during the random masking procedure\\nwe chose the 4-th token (which corresponding to\\nhairy), our masking procedure can be further il-\\nlustrated by\\n• 80% of the time: Replace the word with the\\n[MASK] token, e.g., my dog is hairy →\\nmy dog is [MASK]\\n• 10% of the time: Replace the word with a\\nrandom word, e.g.', 'content_type': 'text', 'score': 0.5378384174439171, 'meta': {'_split_id': 50}, 'embedding': None, 'id': '89d247cb29ec43b5af9cae2887341adb'}>,\n",
      "               <Document: {'content': ',\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshuffled sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks—\\nwhether they involve single text or text pairs—by\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspecific inputs and outputs into BERT and fine-\\ntune all the parameters end-to-end. ', 'content_type': 'text', 'score': 0.5376305643197242, 'meta': {'_split_id': 19}, 'embedding': None, 'id': '2ed2006c8bd6ebf44a1659b6cef19aff'}>,\n",
      "               <Document: {'content': 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be fine-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspecific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n', 'content_type': 'text', 'score': 0.5369979138801834, 'meta': {'_split_id': 0}, 'embedding': None, 'id': 'c6a0b7e9bce962ba6d411032d8fde7ab'}>,\n",
      "               <Document: {'content': 'The final hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classification\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the final hidden\\nvector of the special [CLS] token as C ∈ RH,\\nand the final hidden vector for the ith input token\\nas Ti ∈ RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n', 'content_type': 'text', 'score': 0.5351872470532576, 'meta': {'_split_id': 13}, 'embedding': None, 'id': 'dda9174e3fe0ca625be37f781714ed56'}>,\n",
      "               <Document: {'content': 'Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspecific architectures. BERT is the first fine-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-specific architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n', 'content_type': 'text', 'score': 0.535146067397478, 'meta': {'_split_id': 4}, 'embedding': None, 'id': '1ddff88697635ab760c1347ff3e63458'}>,\n",
      "               <Document: {'content': 'BERT\\nE\\n[CLS]\\nE\\n1\\nE\\n[SEP]\\n... E\\nN\\nE\\n1\\n’ ... E\\nM\\n’\\nC T\\n1\\nT\\n[SEP]\\n... T\\nN\\nT\\n1\\n’ ... T\\nM\\n’\\n[CLS]\\nTok\\n1\\n[SEP]\\n... Tok\\nN\\nTok\\n1\\n... Tok\\nM\\nQuestion Paragraph\\nBERT\\nE\\n[CLS]\\nE\\n1\\nE\\n2\\nE\\nN\\nC T\\n1\\nT\\n2\\nT\\nN\\nSingle Sentence\\n...\\n...\\nBERT\\nTok 1 Tok 2 Tok N\\n...\\n[CLS]\\nE\\n[CLS]\\nE\\n1\\nE\\n2\\nE\\nN\\nC T\\n1\\nT\\n2\\nT\\nN\\nSingle Sentence\\nB-PER\\nO O\\n...\\n...\\nE\\n[CLS]\\nE\\n1\\nE\\n[SEP]\\nClass\\nLabel\\n... E\\nN\\nE\\n1\\n’ ... E\\nM\\n’\\nC T\\n1\\nT\\n[SEP]\\n... T\\nN\\nT\\n1\\n’ ... T\\nM\\n’\\nStart/End Span\\nClass\\nLabel\\nBERT\\nTok 1 Tok 2 Tok N\\n...\\n[CLS] Tok 1\\n[CLS]\\n[CLS]\\nTok\\n1\\n[SEP]\\n... Tok\\nN\\nTok\\n1\\n... Tok\\nM\\nSentence 1\\n...\\nSentence 2\\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classification task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al.', 'content_type': 'text', 'score': 0.535093391069, 'meta': {'_split_id': 62}, 'embedding': None, 'id': '8095bf432d9370de64021240abe7c370'}>,\n",
      "               <Document: {'content': 'Critically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorflow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/filter size to be 4H,\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n4We note that in the literature the bidirectional Trans-\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., Question, Answer ) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The first\\ntoken of every sequence is always a special clas-\\nsification token ([CLS]). The final hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classification\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. ', 'content_type': 'text', 'score': 0.534560162628906, 'meta': {'_split_id': 12}, 'embedding': None, 'id': '2528eb809ff209042b0b5ca76de41806'}>,\n",
      "               <Document: {'content': 'Both of these prior works used a feature-\\nbased approach — we hypothesize that when the\\nmodel is fine-tuned directly on the downstream\\ntasks and uses only a very small number of ran-\\ndomly initialized additional parameters, the task-\\nspecific models can benefit from the larger, more\\nexpressive pre-trained representations even when\\ndownstream task data is very small.\\n5.3 Feature-based Approach with BERT\\nAll of the BERT results presented so far have used\\nthe fine-tuning approach, where a simple classifi-\\ncation layer is added to the pre-trained model, and\\nall parameters are jointly fine-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere fixed features are extracted from the pre-\\ntrained model, has certain advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore require\\na task-specific model architecture to be added.\\nSecond, there are major computational benefits\\nto pre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper models on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the CoNLL-2003 Named\\nEntity Recognition (NER) task (Tjong Kim Sang\\nand De Meulder, 2003). ', 'content_type': 'text', 'score': 0.534560162628906, 'meta': {'_split_id': 37}, 'embedding': None, 'id': 'b8dbeaee615a62fe8e340a1ff465bced'}>],\n",
      " 'no_ans_gap': 7.803023815155029,\n",
      " 'node_id': 'Reader',\n",
      " 'params': {'Reader': {'top_k': 5}, 'Retriever': {'top_k': 10}},\n",
      " 'query': 'What is BERT?',\n",
      " 'root_node': 'Query'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is BERT?\n",
      "Answers:\n",
      "[   {   'answer': 'conceptually simple and empirically\\npowerful',\n",
      "        'context': 'l task-\\n'\n",
      "                   'specific architecture modifications.\\n'\n",
      "                   'BERT is conceptually simple and empirically\\n'\n",
      "                   'powerful. It obtains new state-of-the-art re-\\n'\n",
      "                   'sults on eleven'},\n",
      "    {   'answer': 'fine-\\ntuning approaches',\n",
      "        'context': 'tion to the architec-\\n'\n",
      "                   'ture differences, BERT and OpenAI GPT are fine-\\n'\n",
      "                   'tuning approaches, while ELMo is a feature-based\\n'\n",
      "                   'approach.\\n'\n",
      "                   'The most comparable e'},\n",
      "    {   'answer': 'bidirectional self-attention',\n",
      "        'context': 'Critically, however, the BERT Transformer uses\\n'\n",
      "                   'bidirectional self-attention, while the GPT Trans-\\n'\n",
      "                   'former uses constrained self-attention where every\\n'\n",
      "                   't'},\n",
      "    {   'answer': 'the first fine-\\ntuning based representation model',\n",
      "        'context': 'y-engineered task-\\n'\n",
      "                   'specific architectures. BERT is the first fine-\\n'\n",
      "                   'tuning based representation model that achieves\\n'\n",
      "                   'state-of-the-art performance on a l'},\n",
      "    {   'answer': 'self-attention mechanism',\n",
      "        'context': 'Parikh et al. (2016); Seo et al. (2017). BERT\\n'\n",
      "                   'instead uses the self-attention mechanism to unify\\n'\n",
      "                   'these two stages, as encoding a concatenated text\\n'\n",
      "                   'pai'}]\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"minimum\" ## Choose from `minimum`, `medium` and `all`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30b59cae7a844251f7e36e84023b83b4380027fe8fb70321965517ad725871b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
